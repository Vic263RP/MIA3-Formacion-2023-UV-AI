{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"Regression Analysis.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"rMzkVEySL3Tm"},"source":["# Regression Analysis"]},{"cell_type":"markdown","metadata":{"id":"xU2goPQtL3Tq"},"source":["In this notebook we will see how regression analysis can help to understand the data behavior and to predict data values (continuous or discrete).\n","We will present four different regression models: \n","1. Linear regression\n","2. Multiple linear regression\n","3. Polynomial regression \n","4. Logistic regression\n","\n","We will introduce the **Ordinary Leasts Squares** and evaluate the modeling results qualitatively by means of **Seaborn visualization tools** and quantitatively by means of the **Statsmodel** toolbox and the **Scikit-learn library**.\n","We will use a variety of real data sets:\n","* Longley dataset of US macroeconomic data\n","* Prediction of the Price of a New Housing Data Market\n","* Diabetes patients data\n","* Winning or Losing Football Team"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"bkyZU7wBL3Tr"},"source":["# Settings\n","import seaborn as sns\n","sns.set_style(\"whitegrid\")\n","sns.set_context(\"notebook\", font_scale=1, rc={\"lines.linewidth\": 2,'font.family': [u'times']})\n","\n","import matplotlib.pylab as plt\n","%matplotlib inline \n","plt.rc('xtick', labelsize=10) \n","plt.rc('ytick', labelsize=10) \n","plt.rc('font', size=12) \n","plt.rc('figure', figsize = (12, 5))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6TpGt02BL3Tt"},"source":["import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"03odiZAVL3Tt"},"source":["## 1. Linear Regression \n"]},{"cell_type":"markdown","metadata":{"id":"_IjHN1faL3Tt"},"source":["### Notation\n","\n","$x_i$ element of a vector, $\\textbf{x}$ column vector, $\\textbf{x'}$ (transpose of $\\textbf{x}$) row vector, $X$ matrix."]},{"cell_type":"markdown","metadata":{"id":"aoXYzvKLL3Tu"},"source":["### From Data to Models\n","\n","All these questions have a common structure: we are asking about one variable $\\textbf{y}$ (*response*) that can be expressed as a combination of one or more (independent) variables $\\textbf{x}_i$ (commonly called *covariates* or *predictors*).\n","\n","The role of regression is to build a model (formula) to predict the response from the covariates."]},{"cell_type":"markdown","metadata":{"id":"WNIFQWIML3Tv"},"source":["### Linear Model\n","\n","The simplest model we can think of is the **linear model**, where the response $\\textbf{y}$ depends linearly from the covariates $\\textbf{x}_i$:\n","\n","$$ \\textbf{y}  =  a_1 \\textbf{x}_1  + \\dots + a_m \\textbf{x}_{m} $$\n","\n","The $a_i$ are termed the *parameters* of the model or the coefficients.\n","\n","This equation can be rewritten in a more compact form as\n","\n","$$ \\textbf{y}  = X \\textbf{w}$$\n","\n","where $$ \\textbf{y} = \\left( \\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{array} \\right), \n"," X = \\left( \\begin{array}{c} x_{11}  \\dots x_{1m} \\\\ x_{21}  \\dots x_{2m}\\\\ \\vdots \\\\ x_{n1}  \\dots x_{nm} \\end{array} \\right),\n"," \\textbf{w} = \\left( \\begin{array}{c} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_m \\end{array} \\right) $$\n"," \n"," **Linear regression** is the technique for creating linear models.\n"," \n","In the **simple** linear regression, with a single variable, we described the relationship between the predictor and the response with a straight line. \n","\n","The model is:\n","$$ \\textbf{y}  =  a_0+ a_1 \\textbf{x}_1 $$\n","\n","The parameter $a_0$ is called the constant term or the *intercept*.\n","\n","**Example**: Does the insurance price depend on the driving experience?\n","Given the following information, the monthly auto insurance prices ($\\textbf{y}$) and driving experiences in years ($\\textbf{x}_{1}$) of a set of n subjects, we can build a linear model to answer this question.\n","We can also predict the monthly auto insurance price for a driver with 10 years of driving experience.\n"]},{"cell_type":"markdown","metadata":{"id":"Ltgbp45kL3Tw"},"source":["Let's generate a set of data to illustrate simple linear regression"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"0PsDekZ5L3Tw"},"source":["X1 = np.random.randn(300, 2)  # Random floats sampled from a univariate ‚Äúnormal‚Äù (Gaussian) distribution\n","A = np.array([[0.6, .4], [.4, 0.6]])\n","X2 = np.dot(X1, A)\n","plt.plot(X2[:, 0], X2[:, 1], \"o\", alpha=0.3) # alpha, blending value, between 0 (transparent) and 1 (opaque).\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zt0Ud-wSL3Tx"},"source":["We can create a linear model to explain the data"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"Tc_729s4L3Tx"},"source":["model=[0+1*x for x in np.arange(-2,3)]\n","plt.plot(X2[:, 0], X2[:, 1], \"o\", alpha=0.3);\n","plt.plot(np.arange(-2,3), model,'r');\n","plt.show()\n","# The red line gives the predicted values of this model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ehv0UJk0L3Ty"},"source":["But there are other linear models. \n"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"sromdR0tL3Ty"},"source":["plt.plot(X2[:, 0], X2[:, 1], \"o\", alpha=0.3);\n","# We can use several parameters and we do not know which is the best model\n","model1=[0+1*x for x in np.arange(-2,3)]\n","model2=[0.3+0.9*x for x in np.arange(-2,3)]\n","model3=[0-0.1*x for x in np.arange(-2,3)]\n","plt.plot(np.arange(-2,3), model1,'r')\n","plt.plot(np.arange(-2,3), model2,'g')\n","plt.plot(np.arange(-2,3), model3,'y')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"StaxXOBeL3Ty"},"source":["#### Which is the best model for a set of samples? \n","Intuitively one wants to minimize the differences between the predicted the values and the actual values...\n"]},{"cell_type":"markdown","metadata":{"id":"pfjxkBo4L3Ty"},"source":["### Ordinary Least Squares\n","\n","Given a model such as:\n","\n","$$\\textbf{y} = a_0+a_1 \\textbf{x}$$\n","\n","Ordinary Least Squares (OLS) is the simplest and most common **estimator** in which the two $a$'s are chosen to minimize the **square of the distance between the predicted values and the actual values**. \n","\n","Given the set of samples $(\\textbf{x},\\textbf{y})$, the objective is to minimize:\n","\n","$$ ||a_0 + a_1 \\textbf{x} -  \\textbf{y} ||^2_2 = \\sum_{j=1}^n (a_0+a_1 x_{j} -  y_j )^2,$$ with respect to $a_0, a_1$.\n","\n","This expression is often called **sum of squared errors of prediction (SSE)**."]},{"cell_type":"markdown","metadata":{"id":"n2yjI2UcL3Tz"},"source":["The OLS can be implemented in python with <code>Scipy.optimize.fmin</code>:\n","\n","https://docs.scipy.org/doc/scipy/reference/optimize.html"]},{"cell_type":"code","metadata":{"id":"tQkhOm8mL3Tz"},"source":["from scipy.optimize import fmin\n","\n","x = np.array([2.2, 4.3, 5.1, 5.8, 6.4, 8.0])\n","y = np.array([0.4, 10.1, 14.0, 10.9, 15.4, 18.5])\n"," \n","# Minimize the sum of squares using a lambda function\n","\n","b0,b1 = fmin(lambda theta, x, y: np.sum((y - theta[0] - theta[1]*x) ** 2), [0,1], args=(x,y)); \n","\n","plt.plot(x, y, 'ro')\n","plt.plot([0,10], [b0, b0+b1*10], alpha=0.8) # Add the regression line, colored in blue\n","for xi, yi in zip(x,y):\n","    plt.plot([xi, xi], [yi, b0+b1*xi], \"k:\")\n","plt.xlim(2, 9); plt.ylim(0, 20)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"peBhI_d3L3Tz"},"source":["We can minimize other criteria, such as the **sum of absolute differences between the predicted values and the actual values**. "]},{"cell_type":"code","metadata":{"id":"hkU0AZkdL3T0"},"source":["sabs = lambda theta, x, y: np.sum(np.abs(y - theta[0] - theta[1]*x))\n","b0,b1 = fmin(sabs, [0,1], args=(x,y))   # minimize the sum of absolute differences\n","plt.plot(x, y, 'ro')\n","plt.plot([0,10], [b0, b0+b1*10]) # Add the regression line, colored in blue\n","for xi, yi in zip(x,y):\n","    plt.plot([xi, xi], [yi, b0+b1*xi], \"k:\")\n","plt.xlim(2, 9); plt.ylim(0, 20)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fS9t67bXL3T0"},"source":["OLS is a popular approach for several reasons. \n","\n","+ For one, it is computationally cheap to calculate the coefficients. \n","+ It is also easier to interpret than more sophisticated models, and in situations where the goal is understanding a simple model in detail, rather than estimating the response well, they can provide insight into what the model captures. \n","+ Finally, in situations where there is a lot of noise, it may be hard to find the true functional form, so a constrained model can perform quite well compared to a complex model which is more affected by noise.\n","\n","The resulting model is represented as follows:\n","\n","$$\\hat{\\textbf{y}} = \\hat{a}_0+\\hat{a}_1 \\textbf{x}$$\n","\n","Here the hats on the variables represent the fact that they are estimated from the data we have available."]},{"cell_type":"markdown","metadata":{"id":"XS1hVh3BL3T0"},"source":["### Linear Regression Visualization: Seaborn\n","\n","Seaborn is a Python data visualization library based on matplotlib that integrates closely with¬†pandas¬†data structures. It provides a high-level interface for drawing attractive and informative statistical graphics:\n","https://seaborn.pydata.org\n","\n","The ``lmplot()`` function from the Seaborn module is intended for exploring linear relationships of different forms in multidimensional datasets. Input data must be in a Pandas ``DataFrame``. To plot, provide the predictor and response variable names along with the dataset.\n","https://seaborn.pydata.org/generated/seaborn.lmplot.html"]},{"cell_type":"markdown","metadata":{"id":"dYLna2ZZL3T0"},"source":["### Linear regression predictions: statsmodels\n","\n","The ``statsmodels`` package provides several different classes that provide different options for linear regression. Getting started with linear regression is quite straightforward with the OLS module:\n","https://www.statsmodels.org/stable/index.html\n","\n","We can perform the regression of the predictor on the response, using the ``sm.OLS`` class and its initialization ``OLS(y, X)`` method. This method takes as an input two array-like objects: $X$ and $\\textbf{y}$. In general, $X$ will either be a numpy array or a pandas data frame with shape ``(n, p)`` where $n$ is the number of data points and $p$ is the number of predictors. $\\textbf{y}$ is either a one-dimensional numpy array or a pandas series of length $n$."]},{"cell_type":"markdown","metadata":{"id":"JwC9L4CiL3T1"},"source":["### DATA SET 1:  Macroeconomic dataset\n","\n","Let's load the Longley dataset of US macroeconomic data from the R datasets website.\n","https://www.statsmodels.org/stable/datasets/generated/longley.html"]},{"cell_type":"code","metadata":{"id":"6WsLTtLmL3T1"},"source":["import pandas as pd\n","pd.options.mode.chained_assignment = None  # default='warn'\n","\n","df = pd.read_csv('http://vincentarelbundock.github.io/Rdatasets/csv/datasets/longley.csv', index_col=0)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9oCLKegrL3T1"},"source":["Macroeconomic data from 1947 to 1962.\n","\n","We will use the variable Total Derived Employment ('Employed') as our response $\\textbf{y}$ and Gross National Product ('GNP') as our predictor $\\textbf{x}$.\n"]},{"cell_type":"code","metadata":{"id":"fES49dFiL3T1"},"source":["sns.lmplot(\"GNP\", \"Employed\", df, height=5.2, aspect=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mwKc9-kxL3T1"},"source":["This plot has two main components. \n","\n","+ The first is a scatterplot, showing the observed datapoints. \n","+ The second is a regression line, showing the estimated linear model relating the two variables. Because the regression line is only an estimate, it is plotted with a 95% confidence band to give an impression of the certainty in the model."]},{"cell_type":"markdown","metadata":{"id":"jt7lb8ZpL3T1"},"source":["### Exercise\n","Explore **graphically** the linear relations of different variables. Are there other highly collinear variables?\n"]},{"cell_type":"code","metadata":{"id":"b05wNH6pL3T2"},"source":["#your solution here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eD7GyMVRL3T2"},"source":["Let's explore the relations of multiple variables using a **scatter plot** of Pandas. The scatter plot is a grid of plots of multiple variables one against the other, showing the relationship of each variable to the others."]},{"cell_type":"code","metadata":{"id":"kaySf9G3L3T2"},"source":["pd.plotting.scatter_matrix(df, figsize=(10.0,10.0));  \n","# Interesting options: 1) hist_kwds={'bins':30}, 2) diagonal='kde', 3) marker='+'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yIFQwphkL3T2"},"source":["Let's build a linear model with the ``statsmodels`` package and evaluate **quantitatively** the predictions\n","\n","We take the single response variable y ('Employed') and store it separately. We also add a constant term to our predictor X ('GNP') so that we fit the intercept of our linear model."]},{"cell_type":"code","metadata":{"id":"zzGf9qVUL3T2"},"source":["import statsmodels.api as sm\n","y = df.Employed  # response\n","X = df.GNP  # predictor\n","# Note: no constant is added by the model unless you are using formulas (!!)\n","X = sm.add_constant(X)  # Adds a constant term to the predictor\n","X.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QHnAmpXWL3T2"},"source":["We can perform the regression of the predictor on the response, using the ``sm.OLS`` class and its initialization ``OLS(y, X)`` method. \n","\n","This method takes as an input two array-like objects: $X$ and $\\textbf{y}$. In general, $X$ will either be a numpy array or a pandas data frame with shape ``(n, p)`` where $n$ is the number of data points and $p$ is the number of predictors. $\\textbf{y}$ is either a one-dimensional numpy array or a pandas series of length $n$."]},{"cell_type":"code","metadata":{"id":"tli3Vo5zL3T3"},"source":["est=sm.OLS(y, X)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KUR6crbAL3T3"},"source":["We then need to fit the model by calling the OLS object‚Äôs fit() method."]},{"cell_type":"code","metadata":{"id":"a3I5VFfoL3T3"},"source":["est = est.fit()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jay_sQG2L3T3"},"source":["We can obtain the predicted values:"]},{"cell_type":"code","metadata":{"id":"2iSbE0RtL3T4"},"source":["est.predict()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5UPtHoAyL3T4"},"source":["We can obtain the estimated parameters, and the goodness of fit (e.g. MSE and R^2): "]},{"cell_type":"code","metadata":{"id":"M-KdNVjjL3T4"},"source":["print(est.params)\n","print('MSE: ',est.mse_total)\n","print('R_squared: ',est.rsquared)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zQYDvrQtL3T4"},"source":["Let's plot the linear model and a summary of regression results: "]},{"cell_type":"code","metadata":{"id":"r4YyTrPZL3T4"},"source":["import numpy as np\n","# We pick 100 hundred points equally spaced from the min to the max \n","X_prime = np.linspace(df.GNP.min(), df.GNP.max(), 100)\n","X_prime = sm.add_constant(X_prime) # add constant as we did before \n","# Now we calculate the predicted values \n","y_hat = est.predict(X_prime) \n","plt.scatter(df.GNP, y, alpha=0.3) \n","# Plot the raw data \n","plt.xlabel(\"Gross National Product\") \n","plt.ylabel(\"Total Employment\") \n","plt.plot(X_prime[:, 1], y_hat, 'r', alpha=0.9) # Add the regression line, colored in red "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6GuV9hwjL3T4"},"source":["est.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iW508oOqL3T4"},"source":["What happens if we force the linear regression to pass through (0,0)?"]},{"cell_type":"code","metadata":{"id":"H02jBoYEL3T5"},"source":["y = df.Employed  # response\n","X = df.GNP  # predictor\n","est2=sm.OLS(y, X).fit()\n","print(est2.params)\n","print('MSE: ',est2.mse_total)\n","print('R_squared: ',est2.rsquared)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QGs7i8GRL3T5"},"source":["import numpy as np\n","# We pick 100 hundred points equally spaced from the min to the max \n","X_prime2 = np.linspace(0, df.GNP.max(), 100)\n","# Now we calculate the predicted values \n","y_hat = est2.predict(X_prime2) \n","plt.scatter(df.GNP, y, alpha=0.3) \n","# Plot the raw data \n","plt.xlabel(\"Gross National Product\") \n","plt.ylabel(\"Total Employment\") \n","plt.plot(X_prime2, y_hat, 'r', alpha=0.9) # Add the regression line, colored in red \n","#plt.axis('equal')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OKUKydn1L3T5"},"source":["### Exercise\n","\n","Look at the coefficient of determinarion (R^2) values obtained for the two approaches. Is there something wrong? Let's double check by computing the R^2 ourselves"]},{"cell_type":"markdown","metadata":{"id":"kAkMbidpL3T5"},"source":["The coefficient $R^2$ is defined as $(1 - \\textbf{u}/\\textbf{v})$, where:\n","+ $\\textbf{u}$ is the residual sum of squares $\\sum (\\textbf{y} - \\hat{\\textbf{y}})^2$, and\n","+ $\\textbf{v}$ is the regression sum of squares $\\sum (\\textbf{y} - \\bar{\\textbf{y}})^2$, where $\\bar{\\textbf{y}}$ is the mean.\n","\n","\n","The best possible score for $R^2$ is 1.0: lower values are worse."]},{"cell_type":"code","metadata":{"id":"lE0NK3G-L3T5"},"source":["#your solution here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"APX3DLqeL3T5"},"source":["Documentation for statsmodel rsquared:\n","https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLSResults.rsquared.html"]},{"cell_type":"markdown","metadata":{"id":"pcp7v2mXL3T5"},"source":["We can also use \"formulas\", a convenience interface for specifying models using formula strings and DataFrames. In this case the intercept is included by default:"]},{"cell_type":"code","metadata":{"id":"k52mFVUOL3T6"},"source":["import statsmodels.formula.api as sm1\n","\n","results_formula = sm1.ols(formula='df.Employed ~ df.GNP', data=df).fit()\n","print(results_formula.params)\n","print('MSE: ',results_formula.mse_total)\n","print('R_squared: ',results_formula.rsquared)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XbAbOrCNL3T6"},"source":["results_without_intercept = sm1.ols(formula='df.Employed ~ df.GNP - 1', data=df).fit()\n","print(results_without_intercept.params)\n","print('MSE: ',results_without_intercept.mse_total)\n","print('R_squared: ',results_without_intercept.rsquared)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3QaX5i5HL3T6"},"source":["### Exercise\n","Develop a linear model to predict 'Unemployed' based on 'GNP'. Provide the parameters of the model and goodness of fit. "]},{"cell_type":"code","metadata":{"id":"rNvNDxU0L3T6"},"source":["## your solution here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aDgkLOYNL3T6"},"source":["## my solution here\n","y = df.Unemployed  # response\n","X = df.GNP  # predictor\n","X = sm.add_constant(X)  # Adds a constant term to the predictor\n","est=sm.OLS(y, X).fit()\n","y_hat = est.predict()\n","u = ((y-y_hat)**2).sum()\n","v = ((y-y.mean())**2).sum()\n","R2=1-u/v\n","print(est.params)\n","print('MSE: ',est.mse_total)\n","print('R_squared: ',est.rsquared)\n","print('R_squared (double check): ',R2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YSrLCY7kL3T6"},"source":["Plot the original data and the predictions in a single graph "]},{"cell_type":"code","metadata":{"id":"yHFriw5iL3T6"},"source":["#your solution here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tLNyB3L8L3T7"},"source":["#my solution here\n","import numpy as np\n","# We pick 100 hundred points equally spaced from the min to the max \n","X_prime = np.linspace(X.GNP.min(), X.GNP.max(), 100)\n","X_prime = sm.add_constant(X_prime) # add constant as we did before \n","# Now we calculate the predicted values \n","y_hat = est.predict(X_prime) \n","plt.scatter(X.GNP, y, alpha=0.3) \n","# Plot the raw data \n","plt.xlabel(\"Gross National Product\") \n","plt.ylabel(\"Total Unemployed\") \n","plt.plot(X_prime[:, 1], y_hat, 'r', alpha=0.9) # Add the regression line, colored in red "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EjHVdHXTL3T7"},"source":["## 2. Multiple Regression and Polynomial Regression\n","In a linear regression with a single variable we describe the relationship between the predictor and the response with a straight line. This case is called *simple* linear regression. In the case of *multiple* linear regression we extend this idea by fitting a p-dimensional hyperplane to our p predictors.\n","\n","$$ \\textbf{y} = a_1 \\textbf{x}_1 + \\dots + a_p \\textbf{x}_p = X \\textbf{w} $$\n"]},{"cell_type":"markdown","metadata":{"id":"S21BE375L3T7"},"source":["Despite its name, linear regression can be used to fit non-linear functions. A linear regression model is linear in the model parameters, not necessarily in the predictors. If you add non-linear transformations of your predictors to the linear regression model, the model will be non-linear in the predictors.\n","\n","$$ \\textbf{y} = a_1 \\phi(\\textbf{x}_1) + \\dots + a_m \\phi(\\textbf{x}_m) $$\n","\n","A very popular non-linear regression technique is *Polynomial Regression*, a technique which models the relationship between the response and the predictors as an n-th order polynomial. We can for example represent a curved relationship between our variables by introducing a cubic model:\n","\n","$$y_i \\approx a_0 + a_1 x_i + a_2 x_i^2 + a_3 x_i^3$$\n","\n","The higher the order of the polynomial the more \"wigglier\" functions you can fit. \n","\n","Using higher order polynomial comes at a price: **computational complexity** and **overfitting**. Overfitting refers to a situation in which the model fits the idiosyncrasies of the training data and loses the ability to generalize from the seen to predict the unseen."]},{"cell_type":"markdown","metadata":{"id":"-NKoat7ML3T7"},"source":["### Exercise\n","\n","Following with the previous example using Macroeconomic data, try to use higher order models to fit GNP with Employed and Unemployed using <code>seaborn.lmplot</code> and the \"order\" parameter "]},{"cell_type":"code","metadata":{"id":"01Wpmib6L3T7"},"source":["# Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FqgqjjOXL3T7"},"source":["# My code here\n","sns.lmplot(\"GNP\", \"Employed\", df, order=3, height=5.2,aspect=2);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KPUusimvL3T7"},"source":["sns.lmplot(\"GNP\", \"Unemployed\", df, order=3, height=5.2,aspect=2);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OeWA72PPL3T8"},"source":["### DATA SET 2: Housing Data\n","\n","To illustrate polynomial regression we will consider the Boston housing dataset. It provides records measurements of 13 attributes of housing markets around Boston, as well as the median price in $1000's. We want to predict the price of a market given a set of attributes.\n"]},{"cell_type":"markdown","metadata":{"id":"gdze_KXnL3T_"},"source":["To start with we load and visualize the data."]},{"cell_type":"code","metadata":{"scrolled":false,"id":"d5kD9VfkL3T_"},"source":["from sklearn import datasets\n","boston = datasets.load_boston()\n","X_boston,y_boston = boston.data, boston.target\n","print('Shape of data:', X_boston.shape, y_boston.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tW4JtF97L3T_"},"source":["print('keys:', boston.keys())\n","print('feature names:',boston.feature_names)\n","print(boston.DESCR)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lhJPe-6IL3T_"},"source":["# Histogram of prices:\n","plt.hist(y_boston) \n","plt.xlabel('price ($1000s)')\n","plt.ylabel('count')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RlmwpTpBL3T_"},"source":["We first consider the task of predicting median house values in the Boston area ``medv`` using as the predictor one of the attributes, for instance, ``lstat``, defined as the \"proportion of lower status of the population\".\n","Seaborn visualization can be used to show this linear relationships easily:"]},{"cell_type":"code","metadata":{"id":"ISddsFLGL3T_"},"source":["# Visualization of the relations between price and LSTAT\n","df_boston = pd.DataFrame(boston.data, columns=boston.feature_names)\n","df_boston['price'] = boston.target\n","sns.lmplot(\"price\", \"LSTAT\", df_boston,height=5.2,aspect=2);\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t4UCuGczL3T_"},"source":["We can see that the relationship between ``medv`` and ``lstat`` is non-linear: the straight line is a poor fit; a better fit can be obtained by including higher order terms.\n","\n","We can visualize the fit of higher order models by changing the \"order\" parameter: "]},{"cell_type":"code","metadata":{"id":"JAJOl2rCL3UA"},"source":["sns.lmplot(\"price\", \"LSTAT\", df_boston, order=2,height=5.2,aspect=2);\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"CqrODoG4L3UA"},"source":["sns.lmplot(\"price\", \"LSTAT\", df_boston, order=3,height=5.2,aspect=2);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BqdXY-MoL3UA"},"source":["We can look at other potential predictors such as the average number of rooms per house ``RM``:"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"5NyKDNSyL3UA"},"source":["# Visualization of the relations between price and RM\n","sns.lmplot(\"price\", \"RM\", df_boston,height=5.2,aspect=2);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Aq2lIFVYL3UA"},"source":["### Exercise\n","Plot the scatter plot of 'RM','AGE','LSTAT' and 'price':"]},{"cell_type":"code","metadata":{"id":"APPaHqmWL3UA"},"source":["#your solution here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"iQewOlG6L3UA"},"source":["#my solution here\n","print(boston.feature_names)\n","indexes=[5,6,12]\n","print(boston.feature_names[indexes])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"w80j4akEL3UA"},"source":["indexes=[5,6,12]\n","df2 = pd.DataFrame(boston.data[:,indexes], columns=boston.feature_names[indexes])\n","df2['price'] = boston.target  #add the target variable as well: price\n","pd.plotting.scatter_matrix(df2, figsize=(10.0,10.0));  \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7hkmnlemL3UB"},"source":["### Multiple Linear Regression Predictions: Scikit-learn\n","\n","Let's make predictions using Scikit-learn. \n","\n","Scikit-learn is a library that provides a variety of both supervised and unsupervised machine learning techniques.\n","\n","Scikit-learn provides an object-oriented interface centered around the concept of an Estimator. \n","\n","The <code>Estimator.fit</code> method sets the state of the estimator based on the *training data*. Usually, the data is comprised of a two-dimensional numpy array $X$ of shape <code>(n_samples, n_predictors)</code> that holds the so-called feature matrix and a one-dimensional numpy array $\\textbf{y}$ that holds the responses. \n","\n","Some estimators allow the user to control the fitting behavior. For example, the <code>sklearn.linear_model.LinearRegression</code> estimator allows the user to specify whether or not to fit an intercept term (by default it is considered). This is done by setting the corresponding constructor arguments of the estimator object.\n"]},{"cell_type":"markdown","metadata":{"id":"xMzG94eQL3UB"},"source":["During the fitting process, the state of the estimator is stored in instance attributes that have a trailing underscore (``'_'``). For example, the coefficients of a ``LinearRegression`` estimator are stored in the attribute ``coef_``:"]},{"cell_type":"code","metadata":{"id":"0_JaauClL3UB"},"source":["from sklearn import linear_model\n","from sklearn.linear_model import LinearRegression\n","clf = LinearRegression(fit_intercept=False) \n","clf.fit ([[0, 1], [1, 1], [2, 1]], [0, 1, 2])  # Perform the fitting\n","clf.coef_"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t8NtLCNjL3UB"},"source":["Estimators that can generate predictions provide a ``Estimator.predict`` method. In the case of regression, ``Estimator.predict`` will return the predicted regression values,  ùê≤ÃÇ\n"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"bM94LGT4L3UB"},"source":["We can evaluate the model fitting by computing the mean squared error ($MSE$) and the coefficient of determination ($R^2$) of the model.\n"]},{"cell_type":"markdown","metadata":{"id":"Do1DAG5YL3UB"},"source":["We will split the data into training set and test set:"]},{"cell_type":"code","metadata":{"id":"aLvDUxj6L3UB"},"source":["from sklearn.model_selection import train_test_split\n","\n","X = pd.DataFrame(df2['LSTAT'])\n","X_train, X_test, y_train, y_test = train_test_split(X, y_boston, test_size=0.3, random_state=0)\n","print('Train and test sizes', X_train.shape, X_test.shape)\n","regr_boston = LinearRegression()\n","regr_boston.fit(X_train, y_train) \n","print('Coeff and intercept:', regr_boston.coef_, regr_boston.intercept_)\n","# Score is R^2: Best possible score is 1.0, lower values are worse.\n","print('Score:', regr_boston.score(X_test, y_test)) \n","print('Training MSE: ', np.mean((regr_boston.predict(X_train) - y_train)**2))\n","print('Test MSE: ', np.mean((regr_boston.predict(X_test) - y_test)**2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"48g3JUfOL3UB"},"source":["### Exercise\n","\n","Check if the goodness of fit increases if we use LSTAT and RM variables as predictors"]},{"cell_type":"code","metadata":{"id":"X2Sc8qVUL3UC"},"source":["#your solution here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TmWSkYfGL3UC"},"source":["#my solution here\n","X = pd.DataFrame(np.c_[df2['LSTAT'], df2['RM']], columns = ['LSTAT','RM'])\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y_boston, test_size=0.3, random_state=0)\n","print('Train and test sizes', X_train.shape, X_test.shape)\n","regr_boston = LinearRegression()\n","regr_boston.fit(X_train, y_train) \n","print('Coeff and intercept:', regr_boston.coef_, regr_boston.intercept_)\n","# Score is R^2: Best possible score is 1.0, lower values are worse.\n","print('Score:', regr_boston.score(X_test, y_test)) \n","print('Training MSE: ', np.mean((regr_boston.predict(X_train) - y_train)**2))\n","print('Test MSE: ', np.mean((regr_boston.predict(X_test) - y_test)**2))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZVBfXdXVL3UC"},"source":["Do we obtain best estimates when using all available variables as inputs? "]},{"cell_type":"code","metadata":{"id":"DE7QfRMxL3UC"},"source":["#your solution here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"94yijpzrL3UC"},"source":["#my solution here\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X_boston, y_boston, test_size=0.4, random_state=0)\n","print('Train and test sizes', X_train.shape, X_test.shape)\n","regr_boston = LinearRegression()\n","regr_boston.fit(X_train, y_train) \n","print('Coeff and intercept:', regr_boston.coef_, regr_boston.intercept_)\n","# Score is R^2: Best possible score is 1.0, lower values are worse.\n","print('Score:', regr_boston.score(X_test, y_test)) \n","print('Training MSE: ', np.mean((regr_boston.predict(X_train) - y_train)**2))\n","print('Test MSE: ', np.mean((regr_boston.predict(X_test) - y_test)**2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YwvepZF9L3UC"},"source":["### Transformer\n","There is a special type of ``Estimator`` called ``Transformer`` which transforms the input data -- e.g. selects a subset of the features or extracts new features based on the original ones.\n","\n","One transformer that we will use here is ``sklearn.preprocessing.StandardScaler``. This transformer centers each predictor in ``X`` to have zero mean and unit variance:\n","\n","Sometimes Standarization is useful:"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"-ElGCebhL3UC"},"source":["from sklearn.preprocessing import StandardScaler\n","# Create the transformer StandardScaler for data and target\n","scalerX = StandardScaler().fit(X_train)\n","scalery = StandardScaler().fit(y_train.reshape(-1,1))\n","\n","print(\"Before transformation:\")\n","print(np.max(y_train), np.min(y_train), np.mean(y_train)) \n","\n","# Normalization of train and test data using mean and variance of the training\n","X_train = scalerX.transform(X_train)\n","y_train = scalery.transform(y_train.reshape(-1,1))\n","X_test = scalerX.transform(X_test)\n","y_test = scalery.transform(y_test.reshape(-1,1))\n","\n","print(\"After transformation:\")\n","print(np.max(y_train), np.min(y_train), np.mean(y_train))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3TE0ovwqL3UC"},"source":["### Exercise\n","Use the scaled data to obtain a linear model for the Boston dataset. Do we get a better fit? "]},{"cell_type":"code","metadata":{"id":"rMirqSppL3UC"},"source":["#your solution here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yuJBxJ31L3UD"},"source":["### DATA SET 3: Diabetes\n","\n","To illustrate linear multiple linear regression we will use the diabetes dataset (from scikit-learn) consists of 10 physiological variables (age, sex, weight, blood pressure) measure on 442 patients, and an indication of disease progression after one year:\n","\n","https://scikit-learn.org/stable/datasets/index.html#diabetes-dataset"]},{"cell_type":"code","metadata":{"id":"_9nlpLU9L3UD"},"source":["from sklearn import datasets\n","diabetes = datasets.load_diabetes()\n","X,y = diabetes.data, diabetes.target\n","print(X.shape, y.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gvSerChhL3UD"},"source":["df3 = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n","pd.plotting.scatter_matrix(df3, figsize=(10.0,10.0)); "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E2mCI0cVL3UD"},"source":["Let's use column 2 to perform a regression:"]},{"cell_type":"code","metadata":{"id":"NKAFvtp_L3UD"},"source":["X2 = X[:,2:3] #we need a matrix\n","X2.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nVZnnaMBL3UD"},"source":["Divide in training and test sets and evaluate the prediction (sklearn) with a simple and a multiple regression model."]},{"cell_type":"code","metadata":{"id":"qom9tJ74L3UD"},"source":["# Remove any possible correlation present in the given input samples\n","from sklearn.utils import shuffle\n","X2,y = shuffle(X2,y,random_state=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LBxbM7_vL3UE"},"source":["# Now we can sequentially split the data into training set and test\n","train_size = 250\n","X_train = X2[:train_size]\n","X_test = X2[train_size:]\n","y_train = y[:train_size]\n","y_test = y[train_size:]\n","print(X_train.shape, X_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NRyUD9A9L3UE"},"source":["Visualize the data using scatter plot (plt.scatter)"]},{"cell_type":"code","metadata":{"id":"ODLK_O3kL3UE"},"source":["# Data visualization\n","plt.scatter(X_train, y_train, color='red', alpha=0.5)\n","plt.scatter(X_test, y_test, color='blue', alpha=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ILHqPESyL3UE"},"source":["from sklearn import linear_model\n","regr1 = LinearRegression()\n","regr1.fit(X_train, y_train) \n","print(regr1.coef_, regr1.intercept_)\n","print('Score:', regr1.score(X_test, y_test))\n","\n","# Calculate the Mean Squared Error on the test set\n","print('Training MSE: ', np.mean((regr1.predict(X_train) - y_train)**2))\n","print('Test MSE: ', np.mean((regr1.predict(X_test) - y_test)**2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uVEIvwkhL3UE"},"source":["# Visualize the linear model prediction\n","plt.scatter(X_train, y_train, color='black')\n","plt.plot(X_train, regr1.predict(X_train), color='red')\n","plt.xlabel('Data')\n","plt.ylabel('Target')\n","\n","plt.scatter(X_test, y_test, color='red'),\n","plt.plot(X_test, regr1.predict(X_test), color='blue')\n","plt.xlabel('Data')\n","plt.ylabel('Target')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K9IQCf5BL3UE"},"source":["### Exercise\n","\n","Divide in training and testing sets and evaluate the prediction using a multiple regression model with the 10 variables."]},{"cell_type":"code","metadata":{"id":"SCfGEkizL3UE"},"source":["#your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"ZAOfXijbL3UE"},"source":["What can we interpret from the regression coefficients? examine their significance using statsmodels"]},{"cell_type":"code","metadata":{"id":"-zSBXWNPL3UE"},"source":["#your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4XE6pGasL3UF"},"source":["## 4. Logistic Regresion \n","\n","**Logistic regression** or logit regression is a type of probabilistic statistical **classification model**. It is also used to predict a binary response from a binary predictor, used for predicting the outcome of a categorical dependent variable (i.e., a class label) based on one or more predictor variables (features). \n"]},{"cell_type":"markdown","metadata":{"id":"TSkTyORDL3UF"},"source":["The logistic function is:\n","\n","$$ f(x) = \\frac{1}{1+e^{- \\lambda x}}$$\n","\n","The logistic function is useful because it can take an input with any value from negative infinity to positive infinity, whereas the output¬†¬†is confined to values between 0 and 1 and hence is interpretable as a probability."]},{"cell_type":"code","metadata":{"scrolled":false,"id":"HIHfnlTcL3UF"},"source":["def logist(x,l):\n","    return 1/(1+np.exp(-l*x))\n","\n","x = np.linspace(-10,10) # 50 points equally spaced from -10 to 10\n","t = logist(x,0.5)\n","y = logist(x,1)\n","z = logist(x,3)\n","plt.plot(x,t, label='lambda=0.5')\n","plt.plot(x,y, label='lambda=1')\n","plt.plot(x,z, label='lambda=3')\n","plt.legend(loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"htXdLQY9L3UF"},"source":["Let's build a logistic regresssion using the Scikit-learn library:\n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n","\n","The solvers implemented in the class LogisticRegression are ‚Äúliblinear‚Äù, ‚Äúnewton-cg‚Äù, ‚Äúlbfgs‚Äù, ‚Äúsag‚Äù and ‚Äúsaga‚Äù. The default is ‚Äúlbfgs‚Äù, which is a reasonably good choice for most cases without a really large dataset. Detailed info here: https://scikit-learn.org/stable/modules/linear_model.html"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"gd4o9VH3L3UF"},"source":["from sklearn import linear_model\n","\n","xmin, xmax = -10, 10\n","n_samples = 100\n","np.random.seed(0)\n","X = np.random.normal(size=n_samples) # Creates 100 random numbers from a normal distribution.\n","y = (X > 0).astype(np.float) \n","X = X[:, np.newaxis] #convert vector to matrix\n","\n","X_test = np.linspace(-10, 10, 300)\n","X_test[:, np.newaxis]\n","\n","# Linear Regression:\n","ols = linear_model.LinearRegression()\n","ols.fit(X, y)\n","plt.plot(X_test, ols.coef_ * X_test + ols.intercept_, color='blue', linewidth=2, label='linear model')\n","\n","# Logistic Regression:\n","clf = linear_model.LogisticRegression(C=10,solver='lbfgs')\n","# C: default = 1.0, intuitively the lambda of the logistic function\n","#'lbfgs': Limited-memory Broyden‚ÄìFletcher‚ÄìGoldfarb‚ÄìShannon\n","clf.fit(X, y)\n","\n","def lr_model(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","loss = lr_model(X_test * clf.coef_ + clf.intercept_).ravel() # in column array\n","plt.plot(X_test, loss, color='red', linewidth=2, label='log reg')\n","#plt.plot(X_test, clf.predict(X_test[:, np.newaxis]).ravel(), color='green', linewidth=2, label='y_hat')\n","plt.scatter(X_test,clf.predict(X_test[:, np.newaxis]).ravel(),color='green',label='y_hat')\n","plt.axhline(0.5, color='grey') # Plot horizontal axis in 0.5\n","\n","plt.scatter(X, y, color='black')\n","\n","plt.legend(loc='lower right')\n","plt.ylabel('y')\n","plt.xlabel('x')\n","\n","plt.ylim(-1, 2)\n","plt.xlim(-4, 4)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UGxN2hL7L3UF"},"source":["\n","## DATA SET 4: Winning or Losing Football Team\n","\n","We want to predict victory or defeat in a football match when we are given the number of goals a team scores.\n","To do this we consider the set of results of the football matches from the Spanish league and we build a classification model with it. <p>\n","\n","We follow the steps:\n","\n","+ Read this file in a pandas DataFrame: http://www.football-data.co.uk/mmz4281/1213/SP1.csv\n","+ Select these columns in a new DataFrame: 'HomeTeam','AwayTeam', 'FTHG', 'FTAG', 'FTR'. (FTHG: Home team goals, FTAG: Away team goals, FTR: H=Home Win, D=Draw, A=Away Win)\n","+ Visualize a scatter plot of FTHG versus FTAG.\n","+ Built a $X$ 1-d predictor with all scores and a $y$ binary variable indicating win or loss.\n","+ Compute and visualize a logistic regression. \n","+ Which is the cut value?\n"]},{"cell_type":"code","metadata":{"id":"cbopcZIML3UG"},"source":["import pylab as pl\n","\n","# Season 2012/2013\n","file = 'files/SP1.csv' # file = 'http://www.football-data.co.uk/mmz4281/1213/SP1.csv'\n","footballData = pd.read_csv(file)\n","s = footballData[['HomeTeam','AwayTeam', 'FTHG', 'FTAG', 'FTR']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"mEP8X4NYL3UG"},"source":["s.head() # (H=Home Win, D=Draw, A=Away Win)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hGD_6_XfL3UG"},"source":["Can we predict if we have a win or loss if we are given a score?"]},{"cell_type":"code","metadata":{"id":"hhlYgkkML3UG"},"source":["# Visualization with scatter the number of goals\"\n","pl.scatter(s.FTHG, s.FTAG, s=100, alpha=0.03) \n","plt.xlabel('Home team goals (FTHG)')\n","plt.ylabel('Away team goals (FTAG)')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MNK1-cNXL3UG"},"source":["# Create two extra columns containing 'W' the number of goals of the winner and 'L' the number of goals of the losser\"\n","def my_f1(row):\n","    return max(row['FTHG'], row['FTAG'])\n","\n","def my_f2(row):\n","    return min(row['FTHG'], row['FTAG'])\n","\n","# Add 2 new columns to the panda:\n","s['W'] = s.apply(my_f1, axis=1)\n","s['L'] = s.apply(my_f2, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lGcUTQu5L3UG"},"source":["s.info()\n","s.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"30NfZvsGL3UG"},"source":["# Create the data and target\n","import numpy as np\n","x1 = s['W'].values\n","y1 = np.ones(len(x1), dtype=np.int)\n","x2 = s['L'].values\n","y2 = np.zeros(len(x2), dtype=np.int)\n","\n","x = np.concatenate([x1,x2])\n","x = x[:, np.newaxis]\n","y = np.concatenate([y1,y2])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oLR6VftxL3UG"},"source":["x = np.concatenate([x1,x2])\n","x = x[:, np.newaxis]\n","x.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ARWm9v_zL3UH"},"source":["# Plot the data \n","plt.scatter(x,y, s=100, alpha=0.03)\n","plt.xlabel('Number of goals')\n","plt.ylabel('Win (1) or loss (0)')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W5XhXtukL3UH"},"source":["### Exercise\n","Compute and visualize the logistic regression. What is the cut value?"]},{"cell_type":"code","metadata":{"id":"kU3xzvCFL3UH"},"source":["#your solution here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"II2ZLoFUL3UH"},"source":[""],"execution_count":null,"outputs":[]}]}
{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uso de la librer√≠a `datasets`\n",
    "Usamos esta librer√≠a para cargar de forma din√°mica un dataset para entrenar/hacer inferencia en modelos de DL.  \n",
    "Se instala con:  \n",
    "\n",
    "`conda install -c huggingface -c conda-forge datasets` \n",
    "\n",
    "Ref: https://huggingface.co/docs/datasets/index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de tomar el tiempo para descargar un conjunto de datos, suele ser √∫til obtener r√°pidamente informaci√≥n general sobre √©l. La informaci√≥n de un conjunto de datos se almacena dentro de DatasetInfo y puede incluir detalles como la descripci√≥n del conjunto de datos, las caracter√≠sticas y el tama√±o del conjunto de datos.\n",
    "\n",
    "Utiliza la funci√≥n `load_dataset_builder()` para cargar un constructor de conjunto de datos y examinar los atributos de un conjunto de datos sin comprometerte a descargarlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset_builder\n",
    "\n",
    "ds_builder = load_dataset_builder(\"glue\", \"mrpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_builder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos cu√°les son los atributos y m√©todos del objeto descargado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[e for e in dir(ds_builder) if not e.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[e for e in dir(ds_builder.info) if not e.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#descripci√≥n del dataset\n",
    "print(ds_builder.info.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#caracter√≠sticas del dataset\n",
    "ds_builder.info.features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una divisi√≥n (split) es un subconjunto espec√≠fico de un conjunto de datos, como entrenamiento y prueba. Enumera los nombres de las divisiones de un conjunto de datos con la funci√≥n `get_dataset_split_names()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import get_dataset_split_names\n",
    "\n",
    "get_dataset_split_names(\"glue\", \"mrpc\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego puedes cargar una divisi√≥n espec√≠fica utilizando el par√°metro \"split\". Cargar una divisi√≥n de un conjunto de datos devuelve un objeto Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"glue\", \"mrpc\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si no especificas una divisi√≥n, ü§ó Datasets devuelve un objeto DatasetDict en su lugar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando un dataset\n",
    "Hay dos tipos de objetos de conjunto de datos, un Dataset regular y luego un **IterableDataset**. Un Dataset proporciona acceso aleatorio r√°pido a las filas y permite la asignaci√≥n de memoria para que la carga de conjuntos de datos grandes utilice solo una cantidad relativamente peque√±a de memoria del dispositivo. Pero para conjuntos de datos realmente, realmente grandes que ni siquiera caben en el disco o en la memoria, un IterableDataset te permite acceder y utilizar el conjunto de datos sin tener que esperar a que se descargue por completo.\n",
    "\n",
    "### Objeto `dataset`\n",
    "Cuando cargas una divisi√≥n de un conjunto de datos, obtienes un objeto Dataset. Puedes hacer muchas cosas con un objeto Dataset, por lo que es importante aprender c√≥mo manipular e interactuar con los datos almacenados en su interior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"glue\", \"mrpc\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[e for e in dir(dataset) if not e.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Indexaci√≥n\n",
    "\n",
    "Un Dataset contiene columnas de datos, y cada columna puede ser un tipo de dato diferente. El √≠ndice, o etiqueta de eje, se utiliza para acceder a ejemplos del conjunto de datos. Por ejemplo, la indexaci√≥n por fila devuelve un diccionario con un ejemplo del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#primera fila del dataset\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#√∫ltima fila del dataset\n",
    "dataset[-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La indexaci√≥n por el nombre de la columna devuelve una lista de todos los valores en esa columna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['sentence1']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes combinar la indexaci√≥n por fila y por nombre de columna para obtener un valor espec√≠fico en una posici√≥n determinada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][\"sentence1\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al rev√©s tambi√©n funciona pero es m√°s lento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"sentence1\"][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slicing\n",
    "El uso de la t√©cnica de \"slicing\" devuelve una rebanada o subconjunto del conjunto de datos, lo cual es √∫til para ver varias filas a la vez. Para hacer un corte (slice) de un conjunto de datos, utiliza el operador \":\" para especificar un rango de posiciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filas entre la 3 y la 6\n",
    "dataset[3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objeto `IterableDataset`\n",
    "\n",
    "Un IterableDataset se carga cuando estableces el par√°metro \"streaming\" en True al cargar un conjunto de datos utilizando la funci√≥n `load_dataset()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterable_dataset = load_dataset(\"rotten_tomatoes\", split=\"train\", streaming=True)\n",
    "for example in iterable_dataset:\n",
    "     print(example)\n",
    "     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterable_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterable_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterable_dataset.dataset_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un IterableDataset itera progresivamente sobre un conjunto de datos de un ejemplo a la vez, por lo que no es necesario esperar a que todo el conjunto de datos se descargue antes de poder usarlo. Como puedes imaginar, esto es muy √∫til para conjuntos de datos grandes que deseas utilizar de inmediato.\n",
    "\n",
    "Sin embargo, esto significa que el comportamiento de un IterableDataset es diferente al de un Dataset regular. No obtienes acceso aleatorio a los ejemplos en un IterableDataset. En su lugar, debes iterar sobre sus elementos, por ejemplo, llamando a `next(iter())` o utilizando un bucle for para obtener el siguiente elemento del IterableDataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(iterable_dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes obtener un subconjunto del conjunto de datos con un n√∫mero espec√≠fico de ejemplos utilizando la funci√≥n IterableDataset.take():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(iterable_dataset.take(3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento\n",
    "\n",
    "Adem√°s de cargar conjuntos de datos, el principal objetivo de ü§ó Datasets es ofrecer una amplia variedad de funciones de preprocesamiento para llevar un conjunto de datos a un formato adecuado para el entrenamiento con tu marco de aprendizaje autom√°tico.\n",
    "\n",
    "Hay muchas formas posibles de preprocesar un conjunto de datos, y todo depende de tu conjunto de datos espec√≠fico. A veces es posible que necesites cambiar el nombre de una columna, y otras veces puede que necesites desenrollar campos anidados. ü§ó Datasets ofrece una forma de hacer la mayor√≠a de estas cosas. Pero en casi todos los casos de preprocesamiento necesitar√°s tokenizar un conjunto de datos de texto.\n",
    "\n",
    "El √∫ltimo paso de preprocesamiento generalmente implica establecer el formato del conjunto de datos para que sea compatible con el formato de entrada esperado por tu marco de aprendizaje autom√°tico.\n",
    "### Tokenizado\n",
    "Cargamos un modelo de tokenizado de la librer√≠a ü§ó Transformers y ejecutamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "dataset = load_dataset(\"rotten_tomatoes\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(dataset[0][\"text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La forma m√°s r√°pida de tokenizar todo tu conjunto de datos es utilizar la funci√≥n `map()`. Esta funci√≥n acelera la tokenizaci√≥n al aplicar el tokenizador a lotes de ejemplos en lugar de ejemplos individuales. Establece el par√°metro batched en True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "dataset = dataset.map(tokenization, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establece el formato de tu conjunto de datos para que sea compatible con tu framework de aprendizaje autom√°tico:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliza la funci√≥n `set_format()` para convertir el formato del dataset a torch y especificar las columnas a formatear. Esta funci√≥n aplica el formato on-the-fly. Despu√©s de convertir a tensores PyTorch, se convierte en un objeto `torch.utils.data.DataLoader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"])\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliza la funci√≥n `to_tf_dataset()` para establecer el formato del conjunto de datos para que sea compatible con TensorFlow. Tambi√©n necesitar√°s importar un objeto de la clase `DataCollator` de ü§ó Transformers para combinar las longitudes de secuencia variables en un solo lote con longitudes iguales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "tf_dataset = dataset.to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    batch_size=2,\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset creado se puede usar en Tensorflow directamente para entrenar/hacer inferencia en un modelo:\n",
    "```python\n",
    "model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=callbacks)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluar las predicciones\n",
    "ü§ó Datasets proporciona varias m√©tricas comunes y espec√≠ficas de procesamiento del lenguaje natural (NLP) para medir el rendimiento de tus modelos. vamos a ver c√≥mo cargar una m√©trica y la utilizarla para evaluar las predicciones de tu modelo.\n",
    "\n",
    "Puedes ver qu√© m√©tricas est√°n disponibles con la funci√≥n `list_metrics()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import list_metrics\n",
    "metrics_list = list_metrics()\n",
    "len(metrics_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es muy f√°cil cargar una m√©trica con ü§ó Datasets. De hecho, te dar√°s cuenta de que es muy similar a cargar un conjunto de datos. Puedes cargar una m√©trica desde el Hub con la funci√≥n load_metric():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric('glue', 'mrpc')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objeto `Metric`\n",
    "Antes de comenzar a utilizar un objeto Metric, es importante conocerlo un poco mejor. Al igual que con un conjunto de datos, puedes obtener informaci√≥n b√°sica sobre una m√©trica. Por ejemplo, accedemos al par√°metro `inputs_description` en `datasets.MetricInfo` para obtener m√°s informaci√≥n sobre el formato de entrada esperado de una m√©trica y algunos ejemplos de uso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metric.inputs_description)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular las m√©tricas\n",
    "Una vez que hayas cargado una m√©trica, est√°s listo para usarla para evaluar las predicciones de un modelo. Proporciona las predicciones del modelo y las referencias a la funci√≥n `compute()`:\n",
    "```python\n",
    "model_predictions = model(model_inputs)\n",
    "final_score = metric.compute(predictions=model_predictions, references=gold_references)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creaci√≥n de un dataset\n",
    "A veces, es posible que necesites crear un conjunto de datos si est√°s trabajando con tus propios datos. Crear un conjunto de datos con ü§ó Datasets te brinda todas las ventajas de la biblioteca: carga y procesamiento r√°pido, capacidad de trabajar con conjuntos de datos enormes, asignaci√≥n de memoria y m√°s. Puedes crear f√°cil y r√°pidamente un conjunto de datos con enfoques de bajo c√≥digo de ü§ó Datasets, lo que reduce el tiempo necesario para comenzar a entrenar un modelo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creaci√≥n a partir de archivos locales\n",
    "Puedes crear un conjunto de datos a partir de archivos locales especificando la ruta a los archivos de datos. Hay dos formas de crear un conjunto de datos utilizando los m√©todos from_:\n",
    "\n",
    "El m√©todo `from_generator()` es la forma m√°s eficiente en t√©rminos de memoria para crear un conjunto de datos a partir de un generador debido al comportamiento iterativo de los generadores. Esto es especialmente √∫til cuando trabajas con un conjunto de datos realmente grande que puede no caber en memoria, ya que el conjunto de datos se genera progresivamente en disco y luego se asigna a memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "def gen():\n",
    "    yield {\"pokemon\": \"bulbasaur\", \"type\": \"grass\"}\n",
    "    yield {\"pokemon\": \"squirtle\", \"type\": \"water\"}\n",
    "ds = Dataset.from_generator(gen)\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un `IterableDataset` basado en generador necesita ser iterado con un bucle `for`, por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import IterableDataset\n",
    "ds = IterableDataset.from_generator(gen)\n",
    "for example in ds:\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El m√©todo `from_dict()` es una forma directa de crear un dataset a partir de un diccionario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "ds = Dataset.from_dict({\"pokemon\": [\"bulbasaur\", \"squirtle\"], \"type\": [\"grass\", \"water\"]})\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un ejemplo completo del uso de un dataset en una tarea de clasificaci√≥n se puede encontrar en https://huggingface.co/docs/transformers/tasks/sequence_classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo\n",
    "Creamos un dataset a partir de un objeto generador que devuelve los documentos de un archivo de texto l√≠nea a l√≠nea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_texts(fname):\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            yield ({'texto': line, 'long': len(line)})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos = build_texts(\"lee_background.cor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in textos:\n",
    "    print(t)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_generator(build_texts, gen_kwargs={\"fname\": 'lee_background.cor'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este dataset en realidad tambi√©n se podr√≠a haber creado con el m√©todo `from_file` de la librer√≠a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset('text', data_files = {'train': 'lee_background.cor'}, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9427b6968756065d1bd05eea5b80a14e2325e56f8688553e9d79bdc9b1540118"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
